Q: why does token prediction result in intelligence

A(google ai mode):
Token prediction generates intelligence because predicting the next word (token) in a vast sea of data forces the model to learn deep, complex patterns, concepts, and relationships about the world, physics, human behavior, and logic embedded in that data, effectively building internal "world models" to make accurate predictions, a process mirrored in how humans learn and understand by predicting future events. While simple on the surface, this task scales to encompass understanding causality, context, and generating coherent, meaningful sequences that mimic reasoning and understanding. 
How it works (The Core Idea)
Learning Patterns: By constantly guessing the next token (word, sub-word) in human-generated text, the model learns statistical probabilities but also much deeper structures, from grammar to factual knowledge.
Building Internal Models: To predict accurately over long sequences, the model must develop internal representations (latent spaces) of concepts, entities, and their relationships, essentially forming a "world model".
Emergent Abilities: This predictive capability, when scaled, leads to emergent behaviors like reasoning, summarization, and question-answering, as these tasks can be framed as predicting the next relevant token. 
Why it's considered "intelligence"
Fundamental to Cognition: Some theories suggest that prediction is a core function of intelligence, both human and artificial, allowing us to navigate the world by anticipating what comes next.
Simulating Understanding: The model isn't just spitting out words; it's generating sequences that reflect a sophisticated understanding of context, enabling it to create novel, relevant outputs.
Beyond Simple Statistics: It's not just about the next word; it's about mapping patterns of entire paragraphs, understanding nuances, and using complex relational information, similar to how the brain works. 
Limitations & Nuances
"Just" Prediction? Critics argue it's still statistical mimicry and lacks true understanding or consciousness, though its outputs often seem intelligent.
Fine-Tuning is Key: Early models relied purely on this, but later generations use techniques like Chain-of-Thought (CoT) prompting (showing "work") to improve reasoning beyond basic next-token prediction. 